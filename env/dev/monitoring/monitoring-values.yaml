# Production-ready monitoring configuration with persistence
# Uses local PersistentVolumes for data storage

ingress:
  enabled: true
  grafana:
    path: /grafana
  prometheus:
    path: /prometheus  
  alertmanager:
    path: /alertmanager

kube-prometheus-stack:
  # Grafana - with persistence
  grafana:
    adminPassword: "monitoring-admin123"
    
    resources:
      limits:
        cpu: 300m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 256Mi
    
    # ENABLE persistence with proper StorageClass
    persistence:
      enabled: true
      size: 2Gi
      storageClassName: local-storage
      accessModes:
        - ReadWriteOnce
    
    # Enable sidecar for dashboards
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        
    # Useful plugins for monitoring
    plugins:
      - grafana-piechart-panel
      - grafana-worldmap-panel
        
    grafana.ini:
      server:
        root_url: http://192.168.1.100/grafana
      analytics:
        check_for_updates: false
        reporting_enabled: false
      log:
        level: info
      paths:
        data: /var/lib/grafana/data
        logs: /var/log/grafana
        plugins: /var/lib/grafana/plugins
        provisioning: /etc/grafana/provisioning

  # Prometheus - with persistence
  prometheus:
    enabled: true
    
    prometheusSpec:
      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      
      # Proper retention with persistence
      retention: 15d
      retentionSize: 8GB
      
      # ENABLE storage with proper StorageClass
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: local-storage
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      
      scrapeInterval: 30s
      evaluationInterval: 30s
      
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      
      # Comprehensive scraping configuration
      additionalScrapeConfigs:
        # Microservices in all environments
        - job_name: 'microservices-dev'
          scrape_interval: 30s
          kubernetes_sd_configs:
            - role: pod
              namespaces:
                names: ['dev']
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - source_labels: [__meta_kubernetes_namespace]
              target_label: environment
            - source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_container_name]
              target_label: container

        # ArgoCD metrics
        - job_name: 'argocd'
          scrape_interval: 30s
          kubernetes_sd_configs:
            - role: service
              namespaces:
                names: ['argocd']
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
              action: keep
              regex: argocd-metrics

      # Custom alerting rules
      additionalPrometheusRulesMap:
        microservices.rules:
          groups:
          - name: microservices
            rules:
            - alert: MicroserviceDown
              expr: up{job="microservices-dev"} == 0
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: "Microservice {{ $labels.container }} is down"
                description: "{{ $labels.container }} in {{ $labels.environment }} has been down for more than 2 minutes."
                
            - alert: HighMemoryUsage
              expr: (container_memory_usage_bytes{namespace="dev"} / container_spec_memory_limit_bytes) > 0.8
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High memory usage in {{ $labels.container }}"
                description: "{{ $labels.container }} is using {{ $value | humanizePercentage }} of available memory."
                
            - alert: HighCPUUsage
              expr: rate(container_cpu_usage_seconds_total{namespace="dev"}[5m]) > 0.8
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High CPU usage in {{ $labels.container }}"

  # AlertManager - with persistence
  alertmanager:
    enabled: true
    
    alertmanagerSpec:
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 100m
          memory: 128Mi
          
      # ENABLE storage with proper StorageClass
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: local-storage
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
      
      retention: 168h  # 1 week
      
      # AlertManager configuration
      config:
        global:
          smtp_smarthost: 'localhost:587'
          smtp_from: 'alertmanager@cluster.local'
          
        route:
          group_by: ['alertname', 'environment']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 1h
          receiver: 'web.hook'
          
        receivers:
        - name: 'web.hook'
          webhook_configs:
          - url: 'http://127.0.0.1:5001/'

  # Prometheus Operator
  prometheusOperator:
    resources:
      limits:
        cpu: 300m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 256Mi

  # Node Exporter - полные метрики узлов
  nodeExporter:
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 64Mi

  # kube-state-metrics - полные метрики кластера
  kubeStateMetrics:
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

  # Comprehensive alerting rules
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false  # У нас не etcd кластер
      general: true
      k8s: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      node: true
      prometheus: true

# ServiceMonitor для автоматического обнаружения метрик
serviceMonitor:
  enabled: true
